{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow explained to myself\n",
    "Based on the 'Deep MNIST tutorial for experts' tutorial. <br>\n",
    "Some things were not crystal clear when reading the tutorial, so I figured I might write my own tutorial so I can quickly look up things I forgot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train = pd.read_csv(\"/Users/edouardcuny/Downloads/train.csv\")\n",
    "x_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "x_train = x_train.as_matrix()\n",
    "x_train = x_train/255\n",
    "\n",
    "y_train = y_train.as_matrix()\n",
    "x_train = x_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "\n",
    "# y_train from labels to one hot\n",
    "y = np.zeros([y_train.shape[0],10])\n",
    "for i in range(y.shape[0]):\n",
    "    y[i, int(y_train[i])]=1\n",
    "y_train = y\n",
    "\n",
    "\n",
    "# TEST\n",
    "test = pd.read_csv(\"/Users/edouardcuny/Downloads/test.csv\")\n",
    "x_test = test.as_matrix()\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions to iterate through the training data. <br>\n",
    "Need to do this since data is imported in a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_______ ITERATE THROUGH BATCH _______ #\n",
    "\n",
    "def next_batch_x_train(batch_size):\n",
    "    global index_batch_x_train\n",
    "    array = x_train\n",
    "    \n",
    "    if batch_size > array.shape[0]:\n",
    "        raise IndexError\n",
    "    \n",
    "    if (index_batch_x_train+1)*batch_size > array.shape[0]:\n",
    "        index = index_batch_x_train\n",
    "        index_batch_x_train = 0\n",
    "        return array[index*batch_size:,:]\n",
    "    \n",
    "    else:\n",
    "        index = index_batch_x_train\n",
    "        index_batch_x_train += 1\n",
    "        return array[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "def next_batch_y_train(batch_size):\n",
    "    global index_batch_y_train\n",
    "    array = y_train\n",
    "    \n",
    "    if batch_size > array.shape[0]:\n",
    "        raise IndexError\n",
    "    \n",
    "    if (index_batch_y_train+1)*batch_size > array.shape[0]:\n",
    "        index = index_batch_y_train\n",
    "        index_batch_y_train = 0\n",
    "        return array[index*batch_size:,:]\n",
    "    \n",
    "    else:\n",
    "        index = index_batch_y_train\n",
    "        index_batch_y_train += 1\n",
    "        return array[index*batch_size:(index+1)*batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward\n",
    "\n",
    "First we begin by creating custom functions to generate the variables to have cleaner code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating x (the input) is pretty straighforward. <br>\n",
    "It's a placeholder. <br>\n",
    "Its size is [None, 784] : \n",
    "* None = flexible number of examples; None means that it can be anything\n",
    "* 784 = image size\n",
    "\n",
    "\n",
    "Then we reshape x to pass it in the convolutional layer. <br>\n",
    "Expected format is [batch, in_height, in_width, in_channels] with : \n",
    "* **batch** is the number of examples we want it to be flexible so : **-1**\n",
    "* **in_height** is the height of the picture sqrt(784) = **28**\n",
    "* **in_width** = **28**\n",
    "* **in_channels** is the number of input channels / feature maps = **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First convolutional layer\n",
    "\n",
    "Then we set the shape of the variables. <br>\n",
    "There is one bias per feature map (we have 32 feature maps) <br>\n",
    "\n",
    "The weight/filter is in the format [filter_height, filter_width, in_channels, out_channels] <br>\n",
    "We're doing a 5x5 filter with 32 feature maps so :\n",
    "* **filter_height** = 5\n",
    "* **filter_height** = 5\n",
    "* **in_channels** = 1\n",
    "* **out_channels** = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_conv1 = bias_variable([32])\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our first convolutional layer. <br>\n",
    "\n",
    "A word about the strides. <br>\n",
    "It defines how we iterate through x so the strides follow the same format : [batch, in_height, in_width, in_channels]\n",
    "\n",
    "* We're using a stride of one for the convolutional layer. <br>\n",
    "* And a 2x2 max pooling layer with no overlap (stride = window/ksize)\n",
    "\n",
    "As always we define two functions to make the code cleaner. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute our outputs (we're using relu activation functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second convolutional layer\n",
    "Nothing to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densely connected layer\n",
    "We now do a fully connected layer taking all the neurons and outputing 1024 neurons. <br> \n",
    "* Our image has a size of 7x7 (size is unchanged after the convolutional part but is reduced by 2 by the pooling step). \n",
    "* We have 64 feature maps/\n",
    "\n",
    "We have thus have 7x7x64 input neurons and 1024 output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the output of the second maxpooling layer. <br>\n",
    "As usual the -1 indicated that we can any number of input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Dropout layer with a probabilty that is a parameter of our network. <br>\n",
    "You can see the tensorflow workflow as one big function and placeholders are arguments of this function. <br>\n",
    "We want the probability of a neuron to be dropped out to be an argument so its a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout layer\n",
    "Nothing to say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Optimizer\n",
    "We use a cross entropy cost minimized by an Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and scoring everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /201 ### ELAPSED TIME =  0 minutes\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# SESSION TENSORFLOW\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# TRAIN\n",
    "batch_size = 50\n",
    "epochs = 24\n",
    "train_size = train.shape[0] \n",
    "\n",
    "index_batch_x_train = 0\n",
    "index_batch_y_train = 0\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "#for i in range(int((epochs*train_size)/batch_size)):\n",
    "for i in range(10):\n",
    "  if i%100==0:\n",
    "    print(int(i/100),'/201 ### ELAPSED TIME = ', int((time()-start)/60), 'minutes') \n",
    "  batch_xs = next_batch_x_train(batch_size)\n",
    "  batch_ys = next_batch_y_train(batch_size)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob : 0.5})\n",
    "      \n",
    "# TEST\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: x_train,\n",
    "                                    y_: y_train}))\n",
    "\n",
    "#%% TRAINING IS NOW DONE\n",
    "\n",
    "# FIRST ONE\n",
    "prediction = sess.run(tf.argmax(y_conv,1), feed_dict={x: x_test[:1000,:], keep_prob : 1})\n",
    "prediction = pd.DataFrame(prediction)\n",
    "\n",
    "# MIDDLE ONES\n",
    "i = 1\n",
    "while (i+1)*1000 < x_test.shape[0]-1:\n",
    "  prediction2 = sess.run(tf.argmax(y_conv,1), feed_dict={x: x_test[i*1000:(i+1)*1000,:], keep_prob : 1})\n",
    "  prediction2 = pd.DataFrame(prediction2)\n",
    "  prediction = pd.concat([prediction, prediction2], axis=0)\n",
    "  i+=1\n",
    "\n",
    "# LAST ONE\n",
    "prediction2 = sess.run(tf.argmax(y_conv,1), feed_dict={x: x_test[i*1000:,:], keep_prob : 1})\n",
    "prediction2 = pd.DataFrame(prediction2)\n",
    "prediction = pd.concat([prediction, prediction2], axis=0)\n",
    "\n",
    "prediction[1] = [x+1 for x in range(prediction.shape[0])]\n",
    "prediction = prediction.iloc[:,[1,0]]\n",
    "prediction.columns = ['ImageId', 'Label']\n",
    "prediction.to_csv('/Users/edouardcuny/Desktop/ml/tf/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score\n",
    "\n",
    "Ran this code on another machine. <br>\n",
    "Final score on the Kaggle competition : \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
